<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="">
  <meta name="keywords" content="FDSL, VG-FractalDB">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Formula-Supervised Visual-Geometric Pre-training</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() {
      dataLayer.push(arguments);
    }
    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/cc_logo_1.jpg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Fourmula-Supervised <br> Visual-Geometric Pre-training</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.ryosuke-yamada.net/">Ryosuke Yamada</a>*<sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://kenshohara.github.io/">Kensho Hara</a>*<sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hirokatsukataoka.net/">Hirokatsu Kataoka</a><sup>1</sup>,
            </span>
            <br>
            <span class="author-block">
              <a href="https://scholar.google.co.jp/citations?user=h0SRrTgAAAAJ&hl=ja">Koshi Makihara</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://mmai.tech/">Nakamasa Inoue</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://www.rio.gsic.titech.ac.jp/en/index.html">Rio Yokota</a><sup>1,3</sup>,
            </span>
            <span class="author-block">
              <a href="https://staff.aist.go.jp/yu.satou/">Yutaka Satoh</a><sup>1,2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>National Institute of Advanced Industrial Science and Technology (AIST),</span>
            <span class="author-block"><sup>2</sup>University of Tsukuba,</span>
            <span class="author-block"><sup>3</sup>Tokyo Institute of Technology</span>
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">* Equal contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/ryosuke-yamada/3dfractaldb"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src=".images/abst.pdf"
                type="image/pdf">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">FSVGP</span> is a unified pre-trained model for visual (2D) and geometric (3D) recognition.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">
          Throughout the history of computer vision, while research has explored the integration of images (visual) and point clouds (geometric), many advancements in image and 3D object recognition have tended to process these modalities separately.
          We aim to bridge this divide by integrating images and point clouds on a unified transformer model. 
          This approach integrates the modality-specific properties of images and point clouds and achieves fundamental downstream tasks in image and 3D object recognition on a unified transformer model by learning visual-geometric representations. 
          In this work, we introduce Formula-Supervised Visual-Geometric Pre-training (FSVGP), a novel synthetic pre-training method that automatically generates aligned synthetic images and point clouds from mathematical formulas. 
          Through cross-modality supervision, we enable supervised pre-training between visual and geometric modalities. FSVGP also reduces reliance on real data collection, cross-modality alignment, and human annotation. 
          Our experimental results show that FSVGP pre-trains more effectively than VisualAtom and PC-FractalDB across six tasks: image and 3D object classification, detection, and segmentation. 
          These achievements demonstrate FSVGP's superior generalization in image and 3D object recognition and underscore the potential of synthetic pre-training in visual-geometric representation learning.
        </h2>
        <div class="content has-text-justified">
          <p>
            
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/ryosuke-yamada" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website borrows the template from <a href="https://github.com/nerfies/nerfies.github.io"> Nerfies</a> and <a href="https://hypernerf.github.io/"> HyperNeRF</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
